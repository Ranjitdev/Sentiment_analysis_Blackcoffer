{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af29f0d0",
   "metadata": {},
   "source": [
    "Blackcoffer is an enterprise software and analytics consulting firm based in India and the European Union (Malta). It is a data-driven, technology, and decision science firm focused exclusively on big data and analytics, data-driven dashboards, applications development, information management, and consulting of any kind, from any source, on a massive scale. We are a young and global consulting shop helping enterprises and entrepreneurs to solve big data and analytics, data-driven dashboards, applications development, and information management problems to minimize risk, explore opportunities for future growth, and increase profits more effectively. We provide intelligence, accelerate innovation and implement technology with extraordinary breadth and depth of global insights into big data, data-driven dashboards, application development, and information management for organizations through combining unique, specialist services, and high-level human expertise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce406c25",
   "metadata": {},
   "source": [
    "# Data Extraction and NLP\n",
    "### Test Assignment :\n",
    "    https://drive.google.com/drive/folders/1ltdsXAS_zaZ3hI-q9eze_QCzHciyYAJY\n",
    "## Objective\n",
    "    The objective of this assignment is to extract textual data articles from the given URL and perform text analysis to compute variables that are explained below. \n",
    "    \n",
    "## Data Extraction\n",
    "    Input.xlsx\n",
    "    For each of the articles, given in the input.xlsx file, extract the article text and save the extracted article in a text file with URL_ID as its file name.\n",
    "    While extracting text, please make sure your program extracts only the article title and the article text. It should not extract the website header, footer, or anything other than the article text. \n",
    "\n",
    "* *NOTE: YOU MUST USE PYTHON PROGRAMMING TO EXTRACT DATA FROM THE URLs. YOU CAN USE BEATIFULSOUP, SELENIUM OR SCRAPY, OR ANY OTHER PYTHON LIBRARIES THAT YOU PREFER FOR DATA CRAWLING. \n",
    "\n",
    "## Data Analysis\n",
    "    For each of the extracted texts from the article, perform textual analysis and compute variables, given in the output structure excel file. You need to save the output in the exact order as given in the output structure file, “Output Data Structure.xlsx”\n",
    "\n",
    "* *NOTE: YOU MUST USE PYTHON PROGRAMMING FOR THE DATA ANALYSIS\n",
    "\n",
    "\n",
    "## Variables\n",
    "    Definition of each of the variables given in the “Text Analysis.docx” file.\n",
    "    POSITIVE SCORE\n",
    "    NEGATIVE SCORE\n",
    "    POLARITY SCORE\n",
    "    SUBJECTIVITY SCORE\n",
    "    AVG SENTENCE LENGTH\n",
    "    PERCENTAGE OF COMPLEX WORDS\n",
    "    FOG INDEX\n",
    "    AVG NUMBER OF WORDS PER SENTENCE\n",
    "    COMPLEX WORD COUNT\n",
    "    WORD COUNT\n",
    "    SYLLABLE PER WORD\n",
    "    PERSONAL PRONOUNS\n",
    "    AVG WORD LENGTH\n",
    "\n",
    "## Output Data Structure\n",
    "    Output Variables: \n",
    "    All input variables in “Input.xlsx”\n",
    "    POSITIVE SCORE\n",
    "    NEGATIVE SCORE\n",
    "    POLARITY SCORE\n",
    "    SUBJECTIVITY SCORE\n",
    "    AVG SENTENCE LENGTH\n",
    "    PERCENTAGE OF COMPLEX WORDS\n",
    "    FOG INDEX\n",
    "    AVG NUMBER OF WORDS PER SENTENCE\n",
    "    COMPLEX WORD COUNT\n",
    "    WORD COUNT\n",
    "    SYLLABLE PER WORD\n",
    "    PERSONAL PRONOUNS\n",
    "    AVG WORD LENGTH\n",
    "    Checkout output data structure spreadsheet for the format of your output, i.e. “Output Data Structure.xlsx”.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a46843",
   "metadata": {},
   "source": [
    "## Sentimental Analysis\n",
    "    Sentimental analysis is the process of determining whether a piece of writing is positive, negative, or neutral. The below Algorithm is designed for use in Financial Texts. It consists of steps:\n",
    "\n",
    "## Cleaning using Stop Words Lists\n",
    "    The Stop Words Lists (found in the folder StopWords) are used to clean the text so that Sentiment Analysis can be performed by excluding the words found in Stop Words List. \n",
    "\n",
    "## Creating a dictionary of Positive and Negative words\n",
    "    The Master Dictionary (found in the folder MasterDictionary) is used for creating a dictionary of Positive and Negative words. We add only those words in the dictionary if they are not found in the Stop Words Lists. \n",
    "\n",
    "## Extracting Derived variables\n",
    "    We convert the text into a list of tokens using the nltk tokenize module and use these tokens to calculate the 4 variables described below:\n",
    "\n",
    "* Positive Score: This score is calculated by assigning the value of +1 for each word if found in the Positive Dictionary and then adding up all the values.\n",
    "* Negative Score: This score is calculated by assigning the value of -1 for each word if found in the Negative Dictionary and then adding up all the values. We multiply the score with -1 so that the score is a positive number.\n",
    "* Polarity Score: This is the score that determines if a given text is positive or negative in nature. It is calculated by using the formula: \n",
    "    *Polarity Score = (Positive Score – Negative Score)/ ((Positive Score + Negative Score) + 0.000001) Range is from -1 to +1\n",
    "* Subjectivity Score: This is the score that determines if a given text is objective or subjective. It is calculated by using the formula: \n",
    "    *Subjectivity Score = (Positive Score + Negative Score)/ ((Total Words after cleaning) + 0.000001) Range is from 0 to +1\n",
    "\n",
    "## Analysis of Readability\n",
    "    Analysis of Readability is calculated using the Gunning Fox index formula described below.\n",
    "    Average Sentence Length = the number of words / the number of sentences\n",
    "    Percentage of Complex words = the number of complex words / the number of words \n",
    "    Fog Index = 0.4 * (Average Sentence Length + Percentage of Complex words)\n",
    "\n",
    "## Average Number of Words Per Sentence\n",
    "    The formula for calculating is:\n",
    "    Average Number of Words Per Sentence = the total number of words / the total number of sentences\n",
    "\n",
    "## Complex Word Count\n",
    "    Complex words are words in the text that contain more than two syllables.\n",
    "\n",
    "## Word Count\n",
    "    We count the total cleaned words present in the text by \n",
    "    removing the stop words (using stopwords class of nltk package).\n",
    "    removing any punctuations like ? ! , . from the word before counting.\n",
    "\n",
    "## Syllable Count Per Word\n",
    "    We count the number of Syllables in each word of the text by counting the vowels present in each word. We also handle some exceptions like words ending with \"es\",\"ed\" by not counting them as a syllable.\n",
    "\n",
    "## Personal Pronouns\n",
    "    To calculate Personal Pronouns mentioned in the text, we use regex to find the counts of the words - “I,” “we,” “my,” “ours,” and “us”. Special care is taken so that the country name US is not included in the list.\n",
    "\n",
    "## Average Word Length\n",
    "    Average Word Length is calculated by the formula:\n",
    "    Sum of the total number of characters in each word/Total number of words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d81fbea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gdown\n",
    "import requests\n",
    "import os\n",
    "from docx import Document\n",
    "import warnings\n",
    "from pprint import pprint\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6027b41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = r'https://docs.google.com/spreadsheets/d/1D7QkDHxUSKnQhR--q0BAwKMxQlUyoJTQ/edit?usp=drive_link'\n",
    "objective_file = r'https://docs.google.com/document/d/1wHMJDDvEKksgPRFajZXeycUcldC57lqr/edit?usp=drive_link'\n",
    "output_file = r'https://docs.google.com/spreadsheets/d/1kHcx9epaZKB96zRItudnrDi57cFEndFI/edit?usp=drive_link'\n",
    "text_analysis = r'https://docs.google.com/document/d/11FuBgszZwCSpVWekJ6rR5tBLjU--xfIC/edit?usp=drive_link'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f534738e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_docs(url, file_name):\n",
    "    gdown.download(url, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e1dab5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (uriginal): https://docs.google.com/spreadsheets/d/1D7QkDHxUSKnQhR--q0BAwKMxQlUyoJTQ/edit?usp=drive_link\n",
      "From (redirected): https://docs.google.com/spreadsheets/d/1D7QkDHxUSKnQhR--q0BAwKMxQlUyoJTQ/export?format=xlsx\n",
      "To: C:\\Users\\PythonFiles\\PYcharm\\Sentiment_analysis_Blackcoffer\\notebook\\input.xlsx\n",
      "14.6kB [00:00, 59.2kB/s]\n",
      "Downloading...\n",
      "From (uriginal): https://docs.google.com/document/d/1wHMJDDvEKksgPRFajZXeycUcldC57lqr/edit?usp=drive_link\n",
      "From (redirected): https://docs.google.com/document/d/1wHMJDDvEKksgPRFajZXeycUcldC57lqr/export?format=docx\n",
      "To: C:\\Users\\PythonFiles\\PYcharm\\Sentiment_analysis_Blackcoffer\\notebook\\objective.docx\n",
      "12.5kB [00:00, 153kB/s]\n",
      "Downloading...\n",
      "From (uriginal): https://docs.google.com/spreadsheets/d/1kHcx9epaZKB96zRItudnrDi57cFEndFI/edit?usp=drive_link\n",
      "From (redirected): https://docs.google.com/spreadsheets/d/1kHcx9epaZKB96zRItudnrDi57cFEndFI/export?format=xlsx\n",
      "To: C:\\Users\\PythonFiles\\PYcharm\\Sentiment_analysis_Blackcoffer\\notebook\\output.xlsx\n",
      "15.9kB [00:00, 147kB/s]\n",
      "Downloading...\n",
      "From (uriginal): https://docs.google.com/document/d/11FuBgszZwCSpVWekJ6rR5tBLjU--xfIC/edit?usp=drive_link\n",
      "From (redirected): https://docs.google.com/document/d/11FuBgszZwCSpVWekJ6rR5tBLjU--xfIC/export?format=docx\n",
      "To: C:\\Users\\PythonFiles\\PYcharm\\Sentiment_analysis_Blackcoffer\\notebook\\test_analysis.docx\n",
      "14.6kB [00:00, 415kB/s]\n"
     ]
    }
   ],
   "source": [
    "download_docs(input_file, 'input.xlsx')\n",
    "download_docs(objective_file, 'objective.docx')\n",
    "download_docs(output_file, 'output.xlsx')\n",
    "download_docs(text_analysis, 'test_analysis.docx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00e7f46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "master_dic = r'https://drive.google.com/drive/folders/1YRcVlJO3ZaC78iTC6JcunfZl7Fz4AL8v?usp=drive_link'\n",
    "stopwords = r'https://drive.google.com/drive/folders/1rd7YdoX8tED9mujc0c-6evJU4y7LFc_R?usp=drive_link'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32a4a3c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving folder list\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file 1qqMwc_-ayS38HEOB97osO_nkIxRkbnvh negative-words.txt\n",
      "Processing file 1seAj8G42SmfgUUx8lqVDJofm4Tuh2TOT positive-words.txt\n",
      "Building directory structure completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving folder list completed\n",
      "Building directory structure\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1qqMwc_-ayS38HEOB97osO_nkIxRkbnvh\n",
      "To: C:\\Users\\PythonFiles\\PYcharm\\Sentiment_analysis_Blackcoffer\\notebook\\master_dictionary\\negative-words.txt\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 44.8k/44.8k [00:00<00:00, 502kB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1seAj8G42SmfgUUx8lqVDJofm4Tuh2TOT\n",
      "To: C:\\Users\\PythonFiles\\PYcharm\\Sentiment_analysis_Blackcoffer\\notebook\\master_dictionary\\positive-words.txt\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 19.1k/19.1k [00:00<00:00, 370kB/s]\n",
      "Download completed\n",
      "Retrieving folder list\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file 1aWxyJI0d9MOk59OZ_unfBY5E-Nvg_ezW StopWords_Auditor.txt\n",
      "Processing file 1K-6MjPq5AQg4ICYY6PDfapB7JECUnryD StopWords_Currencies.txt\n",
      "Processing file 13LXnH6vaJhvY4s2ai_2oW2qwongU_iAI StopWords_DatesandNumbers.txt\n",
      "Processing file 1tTDfLXNPxNuUGZXHQkQhW6wPf4Xnivwr StopWords_Generic.txt\n",
      "Processing file 1PnZhcsfjBVxnzwa4N6MrLWf6Kuhhjpdk StopWords_GenericLong.txt\n",
      "Processing file 1RKxMOHzBdLrGuYb7MCJRTKKPwDG9Agbe StopWords_Geographic.txt\n",
      "Processing file 1mBOuggD8AVNFjr9sprLoD2_6mVWAgRGE StopWords_Names.txt\n",
      "Building directory structure completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving folder list completed\n",
      "Building directory structure\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1aWxyJI0d9MOk59OZ_unfBY5E-Nvg_ezW\n",
      "To: C:\\Users\\PythonFiles\\PYcharm\\Sentiment_analysis_Blackcoffer\\notebook\\stopwords\\StopWords_Auditor.txt\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████| 88.0/88.0 [00:00<?, ?B/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1K-6MjPq5AQg4ICYY6PDfapB7JECUnryD\n",
      "To: C:\\Users\\PythonFiles\\PYcharm\\Sentiment_analysis_Blackcoffer\\notebook\\stopwords\\StopWords_Currencies.txt\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 1.76k/1.76k [00:00<00:00, 1.76MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=13LXnH6vaJhvY4s2ai_2oW2qwongU_iAI\n",
      "To: C:\\Users\\PythonFiles\\PYcharm\\Sentiment_analysis_Blackcoffer\\notebook\\stopwords\\StopWords_DatesandNumbers.txt\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████| 832/832 [00:00<?, ?B/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1tTDfLXNPxNuUGZXHQkQhW6wPf4Xnivwr\n",
      "To: C:\\Users\\PythonFiles\\PYcharm\\Sentiment_analysis_Blackcoffer\\notebook\\stopwords\\StopWords_Generic.txt\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████| 722/722 [00:00<?, ?B/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1PnZhcsfjBVxnzwa4N6MrLWf6Kuhhjpdk\n",
      "To: C:\\Users\\PythonFiles\\PYcharm\\Sentiment_analysis_Blackcoffer\\notebook\\stopwords\\StopWords_GenericLong.txt\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 4.16k/4.16k [00:00<00:00, 1.39MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1RKxMOHzBdLrGuYb7MCJRTKKPwDG9Agbe\n",
      "To: C:\\Users\\PythonFiles\\PYcharm\\Sentiment_analysis_Blackcoffer\\notebook\\stopwords\\StopWords_Geographic.txt\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 1.82k/1.82k [00:00<?, ?B/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1mBOuggD8AVNFjr9sprLoD2_6mVWAgRGE\n",
      "To: C:\\Users\\PythonFiles\\PYcharm\\Sentiment_analysis_Blackcoffer\\notebook\\stopwords\\StopWords_Names.txt\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 107k/107k [00:00<00:00, 531kB/s]\n",
      "Download completed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['stopwords\\\\StopWords_Auditor.txt',\n",
       " 'stopwords\\\\StopWords_Currencies.txt',\n",
       " 'stopwords\\\\StopWords_DatesandNumbers.txt',\n",
       " 'stopwords\\\\StopWords_Generic.txt',\n",
       " 'stopwords\\\\StopWords_GenericLong.txt',\n",
       " 'stopwords\\\\StopWords_Geographic.txt',\n",
       " 'stopwords\\\\StopWords_Names.txt']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gdown.download_folder(master_dic, output='master_dictionary')\n",
    "gdown.download_folder(stopwords, output='stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b5ce595",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>37</td>\n",
       "      <td>https://insights.blackcoffer.com/ai-in-healthc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38</td>\n",
       "      <td>https://insights.blackcoffer.com/what-if-the-c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>39</td>\n",
       "      <td>https://insights.blackcoffer.com/what-jobs-wil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40</td>\n",
       "      <td>https://insights.blackcoffer.com/will-machine-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41</td>\n",
       "      <td>https://insights.blackcoffer.com/will-ai-repla...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   URL_ID                                                URL\n",
       "0      37  https://insights.blackcoffer.com/ai-in-healthc...\n",
       "1      38  https://insights.blackcoffer.com/what-if-the-c...\n",
       "2      39  https://insights.blackcoffer.com/what-jobs-wil...\n",
       "3      40  https://insights.blackcoffer.com/will-machine-...\n",
       "4      41  https://insights.blackcoffer.com/will-ai-repla..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_file = pd.read_excel('input.xlsx')\n",
    "input_file.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b17788f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "      <th>POSITIVE SCORE</th>\n",
       "      <th>NEGATIVE SCORE</th>\n",
       "      <th>POLARITY SCORE</th>\n",
       "      <th>SUBJECTIVITY SCORE</th>\n",
       "      <th>AVG SENTENCE LENGTH</th>\n",
       "      <th>PERCENTAGE OF COMPLEX WORDS</th>\n",
       "      <th>FOG INDEX</th>\n",
       "      <th>AVG NUMBER OF WORDS PER SENTENCE</th>\n",
       "      <th>COMPLEX WORD COUNT</th>\n",
       "      <th>WORD COUNT</th>\n",
       "      <th>SYLLABLE PER WORD</th>\n",
       "      <th>PERSONAL PRONOUNS</th>\n",
       "      <th>AVG WORD LENGTH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>37</td>\n",
       "      <td>https://insights.blackcoffer.com/ai-in-healthc...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38</td>\n",
       "      <td>https://insights.blackcoffer.com/what-if-the-c...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>39</td>\n",
       "      <td>https://insights.blackcoffer.com/what-jobs-wil...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40</td>\n",
       "      <td>https://insights.blackcoffer.com/will-machine-...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41</td>\n",
       "      <td>https://insights.blackcoffer.com/will-ai-repla...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   URL_ID                                                URL  POSITIVE SCORE  \\\n",
       "0      37  https://insights.blackcoffer.com/ai-in-healthc...             NaN   \n",
       "1      38  https://insights.blackcoffer.com/what-if-the-c...             NaN   \n",
       "2      39  https://insights.blackcoffer.com/what-jobs-wil...             NaN   \n",
       "3      40  https://insights.blackcoffer.com/will-machine-...             NaN   \n",
       "4      41  https://insights.blackcoffer.com/will-ai-repla...             NaN   \n",
       "\n",
       "   NEGATIVE SCORE  POLARITY SCORE  SUBJECTIVITY SCORE  AVG SENTENCE LENGTH  \\\n",
       "0             NaN             NaN                 NaN                  NaN   \n",
       "1             NaN             NaN                 NaN                  NaN   \n",
       "2             NaN             NaN                 NaN                  NaN   \n",
       "3             NaN             NaN                 NaN                  NaN   \n",
       "4             NaN             NaN                 NaN                  NaN   \n",
       "\n",
       "   PERCENTAGE OF COMPLEX WORDS  FOG INDEX  AVG NUMBER OF WORDS PER SENTENCE  \\\n",
       "0                          NaN        NaN                               NaN   \n",
       "1                          NaN        NaN                               NaN   \n",
       "2                          NaN        NaN                               NaN   \n",
       "3                          NaN        NaN                               NaN   \n",
       "4                          NaN        NaN                               NaN   \n",
       "\n",
       "   COMPLEX WORD COUNT  WORD COUNT  SYLLABLE PER WORD  PERSONAL PRONOUNS  \\\n",
       "0                 NaN         NaN                NaN                NaN   \n",
       "1                 NaN         NaN                NaN                NaN   \n",
       "2                 NaN         NaN                NaN                NaN   \n",
       "3                 NaN         NaN                NaN                NaN   \n",
       "4                 NaN         NaN                NaN                NaN   \n",
       "\n",
       "   AVG WORD LENGTH  \n",
       "0              NaN  \n",
       "1              NaN  \n",
       "2              NaN  \n",
       "3              NaN  \n",
       "4              NaN  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_file = pd.read_excel('output.xlsx')\n",
    "output_file.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e340f87b",
   "metadata": {},
   "source": [
    "# Getting the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a99d32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a90f4e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_urls(input_file) -> List[str]:\n",
    "    urls = []\n",
    "    for i in range(len(input_file)):\n",
    "        url = input_file.iloc[i, 1]\n",
    "        urls.append(url)\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e73cc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_contents(url) -> List:\n",
    "    out_text = []\n",
    "    try:\n",
    "        print(url)\n",
    "        script = requests.get(url)\n",
    "        soup_obj = BeautifulSoup(script.content)\n",
    "        paragraphs = soup_obj.find_all('p')\n",
    "        for i in paragraphs:\n",
    "            out_text.append(i.text)\n",
    "        return out_text\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e7d1fe32",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://insights.blackcoffer.com/man-and-machines-together-machines-are-more-diligent-than-humans-blackcoffe/\n"
     ]
    }
   ],
   "source": [
    "urls = fetch_urls(input_file)\n",
    "my_text = fetch_contents(urls[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "caa79d1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NLP-based Approach for Data Transformation',\n",
       " 'An ETL tool to pull data from Shiphero to Google Bigquery Data Warehouse',\n",
       " 'Plaid Financial Analytics – A Data-Driven Dashboard to generate insights',\n",
       " 'Recommendation Engine for Insurance Sector to Expand Business in the Rural Area',\n",
       " 'Grafana Dashboard – Oscar Awards',\n",
       " 'AutoGPT Setup',\n",
       " 'Playstore & Appstore to Google Analytics (GA) or Firebase to Google Data Studio Mobile App KPI Dashboard',\n",
       " 'Google Local Service Ads LSA API To Google BigQuery to Google Data Studio',\n",
       " 'Rise of telemedicine and its Impact on Livelihood by 2040',\n",
       " 'Rise of e-health and its impact on humans by the year 2030',\n",
       " 'Rise of e-health and its impact on humans by the year 2030',\n",
       " 'Rise of telemedicine and its Impact on Livelihood by 2040',\n",
       " 'AI/ML and Predictive Modeling',\n",
       " 'Solution for Contact Centre Problems',\n",
       " 'How to Setup Custom Domain for Google App Engine Application?',\n",
       " 'Code Review Checklist',\n",
       " 'Where is this disruptive technology taking us? Take it or leave it, disruptive technology always creates new jobs much more than depleted jobs. You might notice certain jobs disappearing but those jobs are the jobs that transform humans to robots, to machines, and the technology is creating machines to replace them.\\xa0 Technology creates the data analysis tools to manipulate and create custom scenarios using artificial intelligence (AI), Big Data and Machine Learning (ML) algorithms to predict and drive consumer behavior. Data Analytics tools, such as Google Analytics , and others are available today for free, and, if used correctly, can help organizations save millions, maybe billions of dollars of sales and marketing.',\n",
       " 'Before I go on, I think it’s best to level set on what constitutes\\xa0machines.\\xa0In the context of this article , machines describe computers and computerized equipment, like robots, that have been programmed to learn, sometimes like humans. Occasionally we call this Artificial Intelligence (AI), other times we call this machine learning, and still other times we call this robotics. And yes, these are technically different things. These bots are more efficient than humans in some specific domains and are growing smarter with each passing day. They can do some really tough tasks which are considered difficult for any human being. \\xa0But, within the broad discussion related to the future of work, these are totally interrelated. Factory floors deploy robots that are increasingly driven by machine learning algorithms such that they can adjust to people working alongside them. A machine can work efficiently only it has abundant data and information about the work which is being imparted daily to them. But with every forward step & advancement in technology, a threat is proliferating, a threat of being replaced on our work front. Every passing day is sealing some jobs for humans all over the globe. Similarly, AI is being used to turn hand-drawn sketches (done by humans) into digital source code.',\n",
       " 'Companies are clearly developing their AI and robotics expertise with the idea that through these technological innovations they’ll be able to',\n",
       " 'Of course, it’s not just machines and creatives working together either. In another example, Amazon has employed more than 100,000 robots in its warehouses to efficiently move things around while it has increased its warehouse workforce by more than 80,000. Humans, in Amazon’s case, do the picking and packing of goods while robots move orders around the giant warehouses, essentially cutting “down on the walking required of workers, making Amazon pickers more efficient and less tired.” Plus, the robots “allow Amazon to pack shelves together like cars in rush-hour traffic because they no longer need aisle space for humans. The greater density of shelf space means more inventory under one roof, which means better selection for customers.”',\n",
       " 'During the next few decades (or maybe sooner), the notion of work and whether it is handled by a human or a virtual being will hinge on predictability. As they are starting to do today, machines will manage the routine while humans take on the unpredictable – tasks that require creativity, problem-solving, and flexibility. In this context, robotics should be seen not only as a means to improve operations efficiency but also to improve the quality of life for workers.',\n",
       " 'Although it is obvious that human factors involved in a work activity impact job automation, it is also true that highly repetitive tasks—and even mechanical ones—are ideal for robots. Besides greater efficiency and speed, automation leads to a lower risk of accidents, greater control and autonomy, and above all, fewer costs for organizations.',\n",
       " 'Although artificial intelligence and machine learning make us believe that robots are endowed with superior intelligence, in fact they don’t yet have the ability to learn from experience and to respond to unknowns. So as things stand, however much processing speed and automatic learning a robot has, it doesn’t beat factors innate to the human brain. Humans are still a very essential part of the process. Think about delivering services to a client. Most customer challenges are routine, but humans play a very important role in addressing new issues, solving them the first time they appear, and then consolidating the process into the system.',\n",
       " 'While machines and humans are placed in proximity, \\xa0robots can be expensive, but this doesn’t apply to all types, especially those based on Robotic Process Automation (RPA), where the development process incorporates algorithms that significantly reduce costs.',\n",
       " 'Moreover, think of how domestic robots—be it a vacuum cleaner, a lawn mower or a pool cleaner—are increasingly part of our daily lives. This level of consumption that robotics has attained makes it affordable to automate tasks in modern homes to obtain greater control, security and comfort.',\n",
       " 'The division between humans and machines has been clear – I’m here, the machine is there – but that boundary is getting fuzzier. Smart prosthetics fuse seamlessly with our bodies, making up for lost limbs or providing additional strength, stability, or resilience, as seen in exoskeletons donned by assembly line workers.',\n",
       " 'We use our smartphones symbiotically, but what if they were integrated directly into our bodies? Think a smartphone in the form of a contact lens capable of transparently delivering augmented reality images straight to the brain. Think it sounds like science fiction? Think again. The first prototypes have already been built.',\n",
       " 'Soon, brain-computer interfaces could become seamless as well, creating a new synergistic relationship between the cloud and us. At that point, the question of who knows what would be moot; you ask me a question and I know the answer. Sometimes that answer will be stored in my own neural circuitry, but most of the time it would come from the connection of my neurons to the web. Our brain’s decision process is influenced by the way it has been “educated” by the cultural context. These external factors are influencing our decision processes to the point that in certain situations, we can legitimately claim that influence has been so strong that our brains can’t be held accountable for the choices made. The point I’m trying to make is that we humans are in symbiosis with our cultural environment and the tools – both physical and conceptual – that we have been taught to use. My guess is that the transformation will be subtle.',\n",
       " 'Practically speaking, robots growing to the point that they take over the world and then start creating smarter, better robots are impractical and should not even be a concern. None of this is expected in the near future, not by a long shot. If you’ve been to an ATM, waited for a PC to boot up after a catastrophic failure, or had a game crash on your X box just when you were about to reach a checkpoint, you understand that we are not in a world where machines do everything perfectly right. Before they can take over all of our jobs, they need to be able to do theirs’ flawlessly; until then, we can depend on humans to mess up our lives.\\xa0',\n",
       " 'This isn’t a win-or-lose situation. We’re going to wind up as a partner to our smarter machines, and that partnership will be fostered by our augmentation through technology. Machines will play an essential role in this augmentation and, as with any successful technology, they will fall below our level of perception. In the end, the revolution will be silent and invisible.',\n",
       " '',\n",
       " 'We provide intelligence, accelerate innovation and implement technology with extraordinary breadth and depth global insights into the big data,data-driven dashboards, applications development, and information management for organizations through combining unique, specialist services and high-lvel human expertise.',\n",
       " 'Contact us: hello@blackcoffer.com',\n",
       " '© All Right Reserved, Blackcoffer(OPC) Pvt. Ltd']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c82940",
   "metadata": {},
   "source": [
    "# Sentimental Analysis\n",
    "    Sentimental analysis is the process of determining whether a piece of writing is positive, negative, or neutral. The below Algorithm is designed for use in Financial Texts. It consists of steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93994ebd",
   "metadata": {},
   "source": [
    "##### Cleaning using Stop Words Lists\n",
    "    The Stop Words Lists (found in the folder StopWords) are used to clean the text so that Sentiment Analysis can be performed by excluding the words found in Stop Words List. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "56d0a2cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\RANJIT\n",
      "[nltk_data]     PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\RANJIT\n",
      "[nltk_data]     PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\RANJIT\n",
      "[nltk_data]     PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "import nltk\n",
    "import re\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60b4826",
   "metadata": {},
   "source": [
    "##### Average Number of Words Per Sentence\n",
    "    Average Number of Words Per Sentence = the total number of words / the total number of sentences\n",
    "##### Word Count\n",
    "    removing the stop words (using stopwords class of nltk package).\n",
    "    removing any punctuations like ? ! , . from the word before counting.\n",
    "##### Average Word Length\n",
    "    Sum of the total number of characters in each word/Total number of words\n",
    "##### Syllable Count Per Word\n",
    "    We count the number of Syllables in each word of the text by counting the vowels present in each word. We also handle some exceptions like words ending with \"es\",\"ed\" by not counting them as a syllable.\n",
    "##### Complex Word Count\n",
    "    Complex words are words in the text that contain more than two syllables.\n",
    "##### Analysis of Readability\n",
    "    Average Sentence Length = the number of words / the number of sentences\n",
    "    Percentage of Complex words = the number of complex words / the number of words \n",
    "    Fog Index = 0.4 * (Average Sentence Length + Percentage of Complex words)\n",
    "##### Personal Pronouns\n",
    "    To calculate Personal Pronouns mentioned in the text, we use regex to find the counts of the words - “I,” “we,” “my,” “ours,” and “us”. Special care is taken so that the country name US is not included in the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "806464e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextProcessorAndCounter:\n",
    "    def __init__(self, sentences: list):\n",
    "        self.sentences = sentences\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_stopwords() -> List[str]:\n",
    "        folder = 'stopwords'\n",
    "        custom_stopwords = []\n",
    "        for i in os.listdir(folder):\n",
    "            with open(os.path.join(folder, i), 'r') as file:\n",
    "                for word in file.readlines():\n",
    "                    word = word.replace('\\n', '').casefold()\n",
    "                    try:\n",
    "                        word0 = word.split('|')[0]\n",
    "                        word1 = word.split('|')[1]\n",
    "                        custom_stopwords.append(word0.strip())\n",
    "                        custom_stopwords.append(word1.strip())\n",
    "                    except:\n",
    "                        custom_stopwords.append(word.strip())\n",
    "        english_stopwords = nltk.corpus.stopwords.words('english')\n",
    "        return custom_stopwords, english_stopwords\n",
    "    \n",
    "    #Clean sentences\n",
    "    def clean_sentences(self) -> List[str]:\n",
    "        custom_stopwords, english_stopwords = self.get_stopwords()\n",
    "        clean_lines = []\n",
    "        for sentence in self.sentences:\n",
    "            line = re.sub('[^a-zA-Z]', ' ', sentence)\n",
    "            new_line = nltk.word_tokenize(line.casefold())\n",
    "            clean_words = []\n",
    "            for word in new_line:\n",
    "                if word not in english_stopwords:\n",
    "                    clean_words.append(word)\n",
    "            clean_words = ' '.join(clean_words)\n",
    "            clean_lines.append(clean_words)\n",
    "        return clean_lines\n",
    "    \n",
    "    #AVG SENTENCE LENGTH\n",
    "    def avg_sentence_length(self) -> int:\n",
    "        length = 0\n",
    "        for i in self.sentences:\n",
    "            length += len(i)\n",
    "        avg_length = int(np.round(length / len(self.sentences)))\n",
    "        return avg_length\n",
    "    \n",
    "    #COMPLEX WORD COUNT\n",
    "    #PERCENTAGE OF COMPLEX WORDS\n",
    "    #SYLLABLE PER WORD\n",
    "    def complex_word(self) -> int:\n",
    "        sentence_list = self.clean_sentences()\n",
    "        pattern = r'(es|ed)$'\n",
    "        vowels = r'[aeiou]'\n",
    "        complex_count = 0\n",
    "        complex_words = []\n",
    "        syllable = 0\n",
    "        word_count = 0\n",
    "        for sentence in sentence_list:\n",
    "            words = nltk.word_tokenize(sentence)\n",
    "            for word in words:\n",
    "                if not re.findall(pattern, word):\n",
    "                    if re.findall(vowels, word):\n",
    "                        vowel_count = len(re.findall(vowels, word))\n",
    "                        syllable += vowel_count\n",
    "                        word_count += 1\n",
    "                        if vowel_count > 2:\n",
    "                            complex_count += 1\n",
    "                            complex_words.append(word)\n",
    "        total_word_count = len(nltk.word_tokenize(' '.join(sentence_list)))\n",
    "        complex_percentage = np.round((complex_count / total_word_count)*100, 2)\n",
    "        avg_syllable = int(np.round(syllable / word_count))\n",
    "        return complex_count, complex_percentage, avg_syllable\n",
    "    \n",
    "    #FOG INDEX\n",
    "    def fog_index(self) -> float:\n",
    "        avg_length = self.avg_sentence_length()\n",
    "        _, complex_percentage, _ = self.complex_word()\n",
    "        value = np.round((avg_length + complex_percentage) * 0.4, 2)\n",
    "        return value\n",
    "    \n",
    "    #PERSONAL PRONOUNS\n",
    "    def personal_pronouns(self) -> str:\n",
    "        pronouns_list = []\n",
    "        pronouns = r'\\b(I|we|my|ours|us|We|My|Ours|Us)\\b'\n",
    "        for sentence in self.sentences:\n",
    "            found = re.findall(pronouns, sentence)\n",
    "            pronouns_list.extend(found)\n",
    "        return ', '.join(pronouns_list)\n",
    "    \n",
    "    #AVG NUMBER OF WORDS PER SENTENCE\n",
    "    #WORD COUNT\n",
    "    #AVG WORD LENGTH\n",
    "    def sentences_words_count(self) -> int:\n",
    "        clean_sentences = self.clean_sentences()\n",
    "        total_sentence_count = len(clean_sentences)\n",
    "        word_count = len(nltk.word_tokenize(' '.join(clean_sentences)))\n",
    "        total_char_count = len(''.join(nltk.word_tokenize(''.join(clean_sentences))))\n",
    "        average_words = int(np.round(word_count / total_sentence_count))\n",
    "        average_word_length = int(np.round(total_char_count / word_count))\n",
    "        return average_words, word_count, average_word_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8f75bb0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "258"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TextProcessorAndCounter(my_text).avg_sentence_length()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d29d0dde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(301, 38.05, 3)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TextProcessorAndCounter(my_text).complex_word()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6622c6ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "118.42"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TextProcessorAndCounter(my_text).fog_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "339c4c1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'us, I, I, we, we, we, us, I, We, us, I, my, my, we, I, we, we, My, we, we, We, We, us'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TextProcessorAndCounter(my_text).personal_pronouns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8a266c01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23, 791, 7)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TextProcessorAndCounter(my_text).sentences_words_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200ca809",
   "metadata": {},
   "source": [
    "##### Positive Score: \n",
    "    This score is calculated by assigning the value of +1 for each word if found in the Positive Dictionary and then adding up all the values.\n",
    "##### Negative Score: \n",
    "    This score is calculated by assigning the value of -1 for each word if found in the Negative Dictionary and then adding up all the values. We multiply the score with -1 so that the score is a positive number.\n",
    "##### Polarity Score: \n",
    "    This is the score that determines if a given text is positive or negative in nature. It is calculated by using the formula: \n",
    "* Polarity Score = (Positive Score – Negative Score)/ ((Positive Score + Negative Score) + 0.000001) Range is from -1 to +1\n",
    "##### Subjectivity Score: \n",
    "    This is the score that determines if a given text is objective or subjective. It is calculated by using the formula: \n",
    "* Subjectivity Score = (Positive Score + Negative Score)/ ((Total Words after cleaning) + 0.000001)\n",
    "Range is from 0 to +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "11fd1f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scores(TextProcessorAndCounter):\n",
    "    def __init__(self, sentences: list):\n",
    "        super().__init__(sentences)\n",
    "        self.sentences = sentences\n",
    "    \n",
    "    @staticmethod\n",
    "    def positive_lib() -> List[str]:\n",
    "        folder = 'master_dictionary/positive-words.txt'\n",
    "        positive_words = []\n",
    "        with open(folder, 'r') as file:\n",
    "            for word in file.readlines():\n",
    "                word = lemmatizer.lemmatize(word.replace('\\n', ''))\n",
    "                positive_words.append(word)\n",
    "        return positive_words\n",
    "    \n",
    "    @staticmethod\n",
    "    def negative_lib() -> List[str]:\n",
    "        folder = 'master_dictionary/negative-words.txt'\n",
    "        negative_words = []\n",
    "        with open(folder, 'r') as file:\n",
    "            for word in file.readlines():\n",
    "                word = lemmatizer.lemmatize(word.replace('\\n', ''))\n",
    "                negative_words.append(word)\n",
    "        return negative_words\n",
    "    \n",
    "    #POSITIVE SCORE\n",
    "    def positive_score(self):\n",
    "        sentences = self.clean_sentences()\n",
    "        positive_words = self.positive_lib()\n",
    "        count = 0\n",
    "        for sentence in self.sentences:\n",
    "            words = nltk.word_tokenize(sentence)\n",
    "            for word in words:\n",
    "                if lemmatizer.lemmatize(word) in positive_words:\n",
    "                    count += 1\n",
    "        return count\n",
    "    \n",
    "    #NEGATIVE SCORE\n",
    "    def negative_score(self):\n",
    "        sentences = self.clean_sentences()\n",
    "        negative_words = self.negative_lib()\n",
    "        count = 0\n",
    "        for sentence in self.sentences:\n",
    "            words = nltk.word_tokenize(sentence)\n",
    "            for word in words:\n",
    "                if lemmatizer.lemmatize(word) in negative_words:\n",
    "                    count += 1\n",
    "        return count\n",
    "    \n",
    "    #POLARITY SCORE\n",
    "    def polarity_score(self):\n",
    "        pos_score = self.positive_score()\n",
    "        neg_score = self.negative_score()\n",
    "        score = np.round((pos_score - neg_score)  / ((pos_score + neg_score) + 0.000001), 2)\n",
    "        return score\n",
    "    \n",
    "    #SUBJECTIVITY SCORE\n",
    "    def subjective_score(self):\n",
    "        _, word_count, _ = self.sentences_words_count()\n",
    "        pos_score = self.positive_score()\n",
    "        neg_score = self.negative_score()\n",
    "        score = np.round((pos_score + neg_score) / (word_count + + 0.000001), 2)\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ee30b9bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Scores(my_text).positive_score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "18fa51ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Scores(my_text).negative_score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c76fd4ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.41"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Scores(my_text).polarity_score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ce47e49d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Scores(my_text).subjective_score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "10e56894",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = Document('objective.docx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c2986ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = TextProcessorAndCounter(my_text)\n",
    "# AVG SENTENCE LENGTH\n",
    "avg_sentence_len = processor.avg_sentence_length()\n",
    "# COMPLEX WORD COUNT\n",
    "# PERCENTAGE OF COMPLEX WORDS\n",
    "# SYLLABLE PER WORD\n",
    "complex_word, percentage_complex, syllable_avg = processor.complex_word()\n",
    "# FOG INDEX\n",
    "fog_index = processor.fog_index()\n",
    "# PERSONAL PRONOUNS\n",
    "personal_pronouns = processor.personal_pronouns()\n",
    "# AVG NUMBER OF WORDS PER SENTENCE\n",
    "# WORD COUNT\n",
    "# AVG WORD LENGTH\n",
    "average_words, word_count, average_word_length = processor.sentences_words_count()\n",
    "score_counter = Scores(my_text)\n",
    "# POSITIVE SCORE\n",
    "p_score = score_counter.positive_score()\n",
    "# NEGATIVE SCORE\n",
    "n_score = score_counter.negative_score()\n",
    "# POLARITY SCORE\n",
    "pol_score = score_counter.polarity_score()\n",
    "# SUBJECTIVITY SCORE\n",
    "sub_score = score_counter.subjective_score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1817f303",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = {\n",
    "    'POSITIVE SCORE': p_score,\n",
    "    'NEGATIVE SCORE': n_score,\n",
    "    'POLARITY SCORE': pol_score,\n",
    "    'SUBJECTIVITY SCORE': sub_score,\n",
    "    'AVG SENTENCE LENGTH': avg_sentence_len,\n",
    "    'PERCENTAGE OF COMPLEX WORDS': percentage_complex,\n",
    "    'FOG INDEX': fog_index,\n",
    "    'AVG NUMBER OF WORDS PER SENTENCE': average_words,\n",
    "    'COMPLEX WORD COUNT': complex_word,\n",
    "    'WORD COUNT': word_count,\n",
    "    'SYLLABLE PER WORD': syllable_avg,\n",
    "    'PERSONAL PRONOUNS': personal_pronouns,\n",
    "    'AVG WORD LENGTH': average_word_length\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3d1c88ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>POSITIVE SCORE</th>\n",
       "      <th>NEGATIVE SCORE</th>\n",
       "      <th>POLARITY SCORE</th>\n",
       "      <th>SUBJECTIVITY SCORE</th>\n",
       "      <th>AVG SENTENCE LENGTH</th>\n",
       "      <th>PERCENTAGE OF COMPLEX WORDS</th>\n",
       "      <th>FOG INDEX</th>\n",
       "      <th>AVG NUMBER OF WORDS PER SENTENCE</th>\n",
       "      <th>COMPLEX WORD COUNT</th>\n",
       "      <th>WORD COUNT</th>\n",
       "      <th>SYLLABLE PER WORD</th>\n",
       "      <th>PERSONAL PRONOUNS</th>\n",
       "      <th>AVG WORD LENGTH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>58</td>\n",
       "      <td>24</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.1</td>\n",
       "      <td>258</td>\n",
       "      <td>38.05</td>\n",
       "      <td>118.42</td>\n",
       "      <td>23</td>\n",
       "      <td>301</td>\n",
       "      <td>791</td>\n",
       "      <td>3</td>\n",
       "      <td>us, I, I, we, we, we, us, I, We, us, I, my, my...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>58</td>\n",
       "      <td>24</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.1</td>\n",
       "      <td>258</td>\n",
       "      <td>38.05</td>\n",
       "      <td>118.42</td>\n",
       "      <td>23</td>\n",
       "      <td>301</td>\n",
       "      <td>791</td>\n",
       "      <td>3</td>\n",
       "      <td>us, I, I, we, we, we, us, I, We, us, I, my, my...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>58</td>\n",
       "      <td>24</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.1</td>\n",
       "      <td>258</td>\n",
       "      <td>38.05</td>\n",
       "      <td>118.42</td>\n",
       "      <td>23</td>\n",
       "      <td>301</td>\n",
       "      <td>791</td>\n",
       "      <td>3</td>\n",
       "      <td>us, I, I, we, we, we, us, I, We, us, I, my, my...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   POSITIVE SCORE  NEGATIVE SCORE  POLARITY SCORE  SUBJECTIVITY SCORE  \\\n",
       "0              58              24            0.41                 0.1   \n",
       "1              58              24            0.41                 0.1   \n",
       "2              58              24            0.41                 0.1   \n",
       "\n",
       "   AVG SENTENCE LENGTH  PERCENTAGE OF COMPLEX WORDS  FOG INDEX  \\\n",
       "0                  258                        38.05     118.42   \n",
       "1                  258                        38.05     118.42   \n",
       "2                  258                        38.05     118.42   \n",
       "\n",
       "   AVG NUMBER OF WORDS PER SENTENCE  COMPLEX WORD COUNT  WORD COUNT  \\\n",
       "0                                23                 301         791   \n",
       "1                                23                 301         791   \n",
       "2                                23                 301         791   \n",
       "\n",
       "   SYLLABLE PER WORD                                  PERSONAL PRONOUNS  \\\n",
       "0                  3  us, I, I, we, we, we, us, I, We, us, I, my, my...   \n",
       "1                  3  us, I, I, we, we, we, us, I, We, us, I, my, my...   \n",
       "2                  3  us, I, I, we, we, we, us, I, We, us, I, my, my...   \n",
       "\n",
       "   AVG WORD LENGTH  \n",
       "0                7  \n",
       "1                7  \n",
       "2                7  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data = pd.DataFrame([result, result, result])\n",
    "new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c1331b2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>37</td>\n",
       "      <td>https://insights.blackcoffer.com/ai-in-healthc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38</td>\n",
       "      <td>https://insights.blackcoffer.com/what-if-the-c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>39</td>\n",
       "      <td>https://insights.blackcoffer.com/what-jobs-wil...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   URL_ID                                                URL\n",
       "0      37  https://insights.blackcoffer.com/ai-in-healthc...\n",
       "1      38  https://insights.blackcoffer.com/what-if-the-c...\n",
       "2      39  https://insights.blackcoffer.com/what-jobs-wil..."
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_data = input_file.iloc[:3, :]\n",
    "my_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "74f39353",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "      <th>POSITIVE SCORE</th>\n",
       "      <th>NEGATIVE SCORE</th>\n",
       "      <th>POLARITY SCORE</th>\n",
       "      <th>SUBJECTIVITY SCORE</th>\n",
       "      <th>AVG SENTENCE LENGTH</th>\n",
       "      <th>PERCENTAGE OF COMPLEX WORDS</th>\n",
       "      <th>FOG INDEX</th>\n",
       "      <th>AVG NUMBER OF WORDS PER SENTENCE</th>\n",
       "      <th>COMPLEX WORD COUNT</th>\n",
       "      <th>WORD COUNT</th>\n",
       "      <th>SYLLABLE PER WORD</th>\n",
       "      <th>PERSONAL PRONOUNS</th>\n",
       "      <th>AVG WORD LENGTH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>37</td>\n",
       "      <td>https://insights.blackcoffer.com/ai-in-healthc...</td>\n",
       "      <td>58</td>\n",
       "      <td>24</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.1</td>\n",
       "      <td>258</td>\n",
       "      <td>38.05</td>\n",
       "      <td>118.42</td>\n",
       "      <td>23</td>\n",
       "      <td>301</td>\n",
       "      <td>791</td>\n",
       "      <td>3</td>\n",
       "      <td>us, I, I, we, we, we, us, I, We, us, I, my, my...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38</td>\n",
       "      <td>https://insights.blackcoffer.com/what-if-the-c...</td>\n",
       "      <td>58</td>\n",
       "      <td>24</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.1</td>\n",
       "      <td>258</td>\n",
       "      <td>38.05</td>\n",
       "      <td>118.42</td>\n",
       "      <td>23</td>\n",
       "      <td>301</td>\n",
       "      <td>791</td>\n",
       "      <td>3</td>\n",
       "      <td>us, I, I, we, we, we, us, I, We, us, I, my, my...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>39</td>\n",
       "      <td>https://insights.blackcoffer.com/what-jobs-wil...</td>\n",
       "      <td>58</td>\n",
       "      <td>24</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.1</td>\n",
       "      <td>258</td>\n",
       "      <td>38.05</td>\n",
       "      <td>118.42</td>\n",
       "      <td>23</td>\n",
       "      <td>301</td>\n",
       "      <td>791</td>\n",
       "      <td>3</td>\n",
       "      <td>us, I, I, we, we, we, us, I, We, us, I, my, my...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   URL_ID                                                URL  POSITIVE SCORE  \\\n",
       "0      37  https://insights.blackcoffer.com/ai-in-healthc...              58   \n",
       "1      38  https://insights.blackcoffer.com/what-if-the-c...              58   \n",
       "2      39  https://insights.blackcoffer.com/what-jobs-wil...              58   \n",
       "\n",
       "   NEGATIVE SCORE  POLARITY SCORE  SUBJECTIVITY SCORE  AVG SENTENCE LENGTH  \\\n",
       "0              24            0.41                 0.1                  258   \n",
       "1              24            0.41                 0.1                  258   \n",
       "2              24            0.41                 0.1                  258   \n",
       "\n",
       "   PERCENTAGE OF COMPLEX WORDS  FOG INDEX  AVG NUMBER OF WORDS PER SENTENCE  \\\n",
       "0                        38.05     118.42                                23   \n",
       "1                        38.05     118.42                                23   \n",
       "2                        38.05     118.42                                23   \n",
       "\n",
       "   COMPLEX WORD COUNT  WORD COUNT  SYLLABLE PER WORD  \\\n",
       "0                 301         791                  3   \n",
       "1                 301         791                  3   \n",
       "2                 301         791                  3   \n",
       "\n",
       "                                   PERSONAL PRONOUNS  AVG WORD LENGTH  \n",
       "0  us, I, I, we, we, we, us, I, We, us, I, my, my...                7  \n",
       "1  us, I, I, we, we, we, us, I, We, us, I, my, my...                7  \n",
       "2  us, I, I, we, we, we, us, I, We, us, I, my, my...                7  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat((my_data, new_data), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2855bcc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af74bb6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
